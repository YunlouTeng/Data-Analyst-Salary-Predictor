# -*- coding: utf-8 -*-
"""data cleaning_for_salary_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WbOfL_SpDhyFkcIYjc-TB9ULhQG0BKob

## 1. Library
"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer


def get_missing_values(DATAFRAME):
  missing_values = pd.DataFrame({
      'missing_values':DATAFRAME.isnull().sum(),
      'percentage':DATAFRAME.isnull().sum()*100/DATAFRAME.shape[0]
  })

  return missing_values.sort_values(by='missing_values', ascending=False)

def get_hourly_salary(x):
  if x is not None:
    x.strip().replace(" ","")
    if x.find('k') > 0:
      return round(float(int(x.lower().replace('k',"")) * 1000/1800),2)
    return float(x)
  return None


def convert_monthly_to_yearly(x):
    if x is not None:
        if x < 8:
            x = x * 12
        else: x
        return x
    return None
        
"""## 2. Import and intergrate the data """

da = pd.read_csv('data collection/data for downstream/df_data_analyst.csv')
ds = pd.read_csv('data collection/data for downstream/df_data_scientist.csv')
bia = pd.read_csv('data collection/data for downstream/df_business_intelligence_analyst.csv')


#combine the tables
data = pd.concat([da,ds,bia])

#clean the columns names
data.rename(columns = {'company_starRating':'CompanyRating', 'CompanyOfferedRole':'JobTitle','salary':'Salary','CompanyRoleLocation':'OfficeLocation',
                       'ListingJobDesc':'JobDescription','RequestedUrl':'JobLink'}, inplace = True)
data.reset_index(drop = True, inplace = True)

data.dropna(subset=['YearFounded','CompanyIndustry','CompanyName'], inplace= True)
data.drop_duplicates(subset=['CompanyName','JobTitle','Salary','OfficeLocation'],inplace= True)


data = data[data['CompanySize'] != 'Unknown']
data = data[data['CompanyType'] != 'Unknown']
data = data[data['CompanyRevenue'] != 'Unknown / Non-Applicable']

data.reset_index(drop = True, inplace= True)

"""## 3. Data dimensions overview"""

print(f'There are {data.shape[0]} data points and {data.shape[1]} features in the dataset')

"""## 4. Clean salary"""

#see distribution
data.Salary.value_counts(dropna=False)
data.Salary = data.Salary.map(lambda x: str(x).split("(",1)[0] if x is not None else None).replace({'\$':''}, regex = True)

data.Salary

#Convert all types of salary into hourly salary

#separate hourly and anaul salary by create a temp column
#data['PerHour'] = data['Salary'].map(lambda x: 1 if str(x).lower().find("per hour") > 0 else 0)

#clean hourly salary data
data.Salary = data.Salary.map(lambda x: str(x).lower().replace("per hour", ""))
data['MinSalary'] = data['Salary'].map(lambda x: x.split("-")[0] if len(x.split("-")) >= 2 else None)
data['MaxSalary'] = data['Salary'].map(lambda x: x.split("-")[1] if len(x.split("-")) >= 2 else x.split("-")[0])
#clean anaul salary data
#working hours: 1800hrs per year
#70 * 1000 /1800



data.MinSalary = data.MinSalary.map(lambda x: get_hourly_salary(x))
data.MaxSalary = data.MaxSalary.map(lambda x: get_hourly_salary(x))


### some job posts provided monthly salary, and we need to convert it to yearly salary
data.MinSalary = data.MinSalary.map(lambda x: convert_monthly_to_yearly(x))

data.MaxSalary = data.MaxSalary.map(lambda x: convert_monthly_to_yearly(x))

#filling missing value by median
min_salary_median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
min_salary_median_imputer = min_salary_median_imputer.fit(data[['MinSalary']])
data['MinSalary'] = min_salary_median_imputer.transform(data[['MinSalary']])

max_salary_median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
max_salary_median_imputer = max_salary_median_imputer.fit(data[['MaxSalary']])
data['MaxSalary'] = max_salary_median_imputer.transform(data[['MaxSalary']])

data['AvgSalary'] = (data.MaxSalary + data.MinSalary) / 2

"""*future work: based on the keyword or the topic of the job description, using regression method to fill the missing value*

## 5. Clean CompanyRating
"""

#filling the missing value with mean
company_rating_mean_value = np.mean(data['CompanyRating'])
data['CompanyRating'].fillna(value = company_rating_mean_value, inplace=True)

"""##6. Clean OfficeLocation

"""

#find locations that dont have the same styles as the majority
data.OfficeLocation[~data.OfficeLocation.str.contains(',')].value_counts()

data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'Atlanta, GA' if str(x) == 'Division of Intuit' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Long Island-Queens' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Midtown New York' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'New York State' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Queen Anne' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Manhattanville' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'Los Angeles, CA' if str(x) == 'California' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Manhattan' else str(x))

#pd.set_option('display.max_colwidth', 50)

data = data[data.OfficeLocation.str.contains(',')]

#separte the cities and states
data['OfficeCity'] = data['OfficeLocation'].map(lambda x: x.split(",")[0])
data['OfficeState'] = data['OfficeLocation'].map(lambda x: x.split(",")[1])
data['OfficeState'] = data['OfficeState'].map(lambda x: x.strip())

state_list = ['WA','GA','MA','IL','CA','NY','NJ']

data = data[data.OfficeState.isin(state_list)]

data.reset_index(drop = True, inplace = True)


"""##Company Age"""

data['CompanyAge'] = data['YearFounded'].map(lambda x: 2023 - x)

"""## Hard Skills"""

#python
data['Python'] = data['JobDescription'].map(lambda x: 1 if 'python' in x.lower() else 0)

#R studio
data['Rstudio'] = data['JobDescription'].map(lambda x: 1 if 'r studio' in x.lower() or 'r-studio' in x.lower() else 0)

#Data Visualization
data['DataVisualization'] = data['JobDescription'].map(lambda x: 1 if 'data visualization' in x.lower() else 0)

#tableau
data['Tableau'] = data['JobDescription'].map(lambda x: 1 if 'tableau' in x.lower() else 0)

#power bi
data['PowerBI'] = data['JobDescription'].map(lambda x: 1 if 'power bi' in x.lower() else 0)

#spark
data['Spark'] = data['JobDescription'].map(lambda x: 1 if 'spark' in x.lower() else 0)

#aws
data['aws'] = data['JobDescription'].map(lambda x: 1 if 'aws' in x.lower() else 0)

#azure
data['Azure'] = data['JobDescription'].map(lambda x: 1 if 'azure' in x.lower() else 0)

data.Spark.value_counts()



"""##Job"""

def seniority(title):
    if 'sr.' in title.lower().strip() or '3' in title.lower().strip() or '2' in title.lower().strip() or 'II' in title.lower().strip() or 'III' in title.lower().strip() or 'senior' in title.lower().strip() or 'sr' in title.lower().strip() or 'lead' in title.lower().strip() or 'principal' in title.lower().strip():
            return 'senior'
    else:
        return 'junior'

def title_simplifier(title):

    if 'scientist' in title.lower().strip():
        return 'data scientist'
    elif 'engineer' in title.lower().strip():
        return 'data engineer'
    elif 'business' in title.lower().strip():
        return 'business analyst'
    elif 'machine learning' in title.lower().strip():
        return 'mle'
    elif 'manager' in title.lower().strip():
        return 'manager'
    elif 'director' in title.lower().strip():
        return 'director'
    else:
        return 'data analyst'

data['JobSeniority'] = data['JobTitle'].apply(seniority)
data['JobSimp'] = data['JobTitle'].apply(title_simplifier)

#expot data 

data.reset_index(drop=True, inplace=True)

data.to_csv('data cleaning/data for downstream/cdata_for_sp.csv')






