# -*- coding: utf-8 -*-
"""data cleaning_for_text_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nrcWYahbWOGdShNMKSXZwHDC8LEm8dts

## 1. Library
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.impute import SimpleImputer

def get_missing_values(DATAFRAME):
  missing_values = pd.DataFrame({
      'missing_values':DATAFRAME.isnull().sum(),
      'percentage':DATAFRAME.isnull().sum()*100/DATAFRAME.shape[0]
  })

  return missing_values.sort_values(by='missing_values', ascending=False)

def get_hourly_salary(x):
  if x is not None:
    x.strip().replace(" ","")
    if x.find('k') > 0:
      return round(float(int(x.lower().replace('k',"")) * 1000/1800),2)
    return float(x)
  return None

"""## 2. Import and intergrate the data """

da = pd.read_csv('data collection/data for downstream/df_data_analyst.csv')
ds = pd.read_csv('data collection/data for downstream/df_data_scientist.csv')
bia = pd.read_csv('data collection/data for downstream/df_business_intelligence_analyst.csv')

#tag each table with job types(search key words)
da['JobType'] = 'data analyst'
ds['JobType'] = 'data scientist'
bia['JobType'] = 'business intelligence analyst'

#combine the tables
data = pd.concat([da,ds,bia])

#clean the columns names
data.rename(columns = {'company_starRating':'CompanyRating', 'CompanyOfferedRole':'JobTitle','salary':'Salary','CompanyRoleLocation':'OfficeLocation',
                       'ListingJobDesc':'JobDescription','RequestedUrl':'JobLink'}, inplace = True)
data.drop_duplicates(subset=['CompanyName','JobTitle','Salary','OfficeLocation'],inplace= True)
data.reset_index(drop = True, inplace= True)

"""## 3. Data dimensions overview"""

print(f'There are {data.shape[0]} data points and {data.shape[1]} features in the dataset')

get_missing_values(data)

plt.figure(figsize=(20,10))
sns.heatmap(data.isnull(), cbar=False)
plt.show()

"""## 4. Clean salary"""

data.Salary

#see distribution
data.Salary.value_counts(dropna=False)
data.Salary = data.Salary.map(lambda x: str(x).split("(",1)[0] if x is not None else None).replace({'\$':''}, regex = True)

#Convert all types of salary into hourly salary

#separate hourly and anaul salary by create a temp column
#data['PerHour'] = data['Salary'].map(lambda x: 1 if str(x).lower().find("per hour") > 0 else 0)

#clean hourly salary data
data.Salary = data.Salary.map(lambda x: str(x).lower().replace("per hour", ""))
data['MinSalary'] = data['Salary'].map(lambda x: x.split("-")[0] if len(x.split("-")) >= 2 else None)
data['MaxSalary'] = data['Salary'].map(lambda x: x.split("-")[1] if len(x.split("-")) >= 2 else x.split("-")[0])
#clean anaul salary data
#working hours: 1800hrs per year
#70 * 1000 /1800

data.MinSalary = data.MinSalary.map(lambda x: get_hourly_salary(x))

data.MaxSalary = data.MaxSalary.map(lambda x: get_hourly_salary(x))

#filling missing value by median
min_salary_median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
min_salary_median_imputer = min_salary_median_imputer.fit(data[['MinSalary']])
data['MinSalary'] = min_salary_median_imputer.transform(data[['MinSalary']])

max_salary_median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')
max_salary_median_imputer = max_salary_median_imputer.fit(data[['MaxSalary']])
data['MaxSalary'] = max_salary_median_imputer.transform(data[['MaxSalary']])

data['AvgSalary'] = (data.MaxSalary + data.MinSalary) / 2

"""*future work: based on the keyword or the topic of the job description, using regression method to fill the missing value*

## 5. Clean CompanyRating
"""

#filling the missing value with mean
company_rating_mean_value = np.mean(data['CompanyRating'])
data['CompanyRating'].fillna(value = company_rating_mean_value, inplace=True)

"""##6. Clean CompanyName&Jobtitle&OfficeLocation"""

pd.set_option('display.max_colwidth', 50)

data.dropna(subset=['CompanyName', 'JobTitle','OfficeLocation'], inplace = True)

#find locations that dont have the same styles as the majority
"""##6. Clean OfficeLocation

"""

#find locations that dont have the same styles as the majority
data.OfficeLocation[~data.OfficeLocation.str.contains(',')].value_counts()

data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'Atlanta, GA' if str(x) == 'Division of Intuit' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Long Island-Queens' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Midtown New York' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'New York State' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Queen Anne' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Manhattanville' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'Los Angeles, CA' if str(x) == 'California' else str(x))
data['OfficeLocation'] = data['OfficeLocation'].map(lambda x: 'New York, NY' if str(x) == 'Manhattan' else str(x))

#pd.set_option('display.max_colwidth', 50)

data = data[data.OfficeLocation.str.contains(',')]

#separte the cities and states
data['OfficeCity'] = data['OfficeLocation'].map(lambda x: x.split(",")[0])
data['OfficeState'] = data['OfficeLocation'].map(lambda x: x.split(",")[1])
data['OfficeState'] = data['OfficeState'].map(lambda x: x.strip())

state_list = ['WA','GA','MA','IL','CA','NY','NJ']

data = data[data.OfficeState.isin(state_list)]

data.reset_index(drop = True, inplace = True)


"""##12. Clean Job description"""

#data['JobDescription'] = data['JobDescription'].map(lambda x: int(1) if len(str(x)) < 1 else str(x))
data['JobDescriptionLength'] = data.JobDescription.str.len()
data = data[data.JobDescriptionLength > 1]

data.reset_index(drop = True, inplace = True)

###output dataset

data.to_csv('data cleaning/data for downstream/cdata_for_ta.csv')


